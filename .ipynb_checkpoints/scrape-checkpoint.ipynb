{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0f5460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from time import sleep\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from satl import Satl\n",
    "import re\n",
    "import json\n",
    "from utils.printer import printer\n",
    "import os\n",
    "import csv\n",
    "\n",
    "base_url = 'https://www.tripadvisor.com'\n",
    "top_activity_url = base_url + '/Attractions-%s-%sActivities-%s.html'\n",
    "# top_restaurants_url = base_url + '/Restaurants-%s-%sActivities-%s.html'\n",
    "\n",
    "cities=[\n",
    "    {'index': 'g187497', 'city': 'Barcelona','country': 'Catalonia'},\n",
    "]\n",
    "\n",
    "\n",
    "def get_page(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:55.0) Gecko/20100101 Firefox/55.0'\n",
    "    }\n",
    "    printer('blue', 'Get', url)\n",
    "    try:\n",
    "        html = requests.get(url, headers=headers)\n",
    "    except:\n",
    "        pass\n",
    "    sleep(1)\n",
    "    soup = BeautifulSoup(html.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "\n",
    "def get_text(item):\n",
    "    if item:\n",
    "        return item.get_text()\n",
    "    return ''\n",
    "\n",
    "\n",
    "def get_poi(url, data_dict, crawler_list, kind):\n",
    "    page = get_page(url)\n",
    "    jsons=page.find(\"script\",type=\"application/ld+json\")\n",
    "    dataj = json.loads(jsons.string)\n",
    "    # print(dataj['@type'])\n",
    "    # data_dict['name'] = dataj['name']\n",
    "    data_dict['images'] = dataj['image']\n",
    "    data_dict['url'] = url\n",
    "    # dir = os.getcwd() + os.sep+'tripAdvisorData/'+dataj['address']['addressLocality']  # save directory\n",
    "    # if not os.path.exists(dir):\n",
    "    #     os.makedirs(dir)\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def get_poi_list(url, crawler_list, kind):\n",
    "    pois = []\n",
    "    page = get_page(url)\n",
    "    if kind == 'things-to-do':\n",
    "        jsons=page.find_all(\"script\",type=\"application/ld+json\")\n",
    "        for j in jsons:        \n",
    "            dataj = json.loads(j.string)\n",
    "            typej=dataj['@type']\n",
    "            if typej=='ItemList':\n",
    "                items=dataj['itemListElement']\n",
    "            \n",
    "    else:\n",
    "        items = page.find_all('div', id=lambda x: x and x.startswith('eatery_'))\n",
    "    # print(dataj)\n",
    "\n",
    "    for item in items:\n",
    "        poi = {'name': item['name']}\n",
    "        url = base_url +item['url']\n",
    "\n",
    "        if is_exists(url):\n",
    "            continue\n",
    "        poi_data = ''\n",
    "        if url.endswith('html.html'):\n",
    "            printer('yellow', 'Not download', url)\n",
    "            continue\n",
    "        try:\n",
    "            poi_data = get_poi(url, poi, crawler_list, kind)\n",
    "        except Exception as e:\n",
    "            msg = '%s - %s' % (url, e)\n",
    "            printer('red', 'Error', msg)\n",
    "        if poi_data:\n",
    "            # set_data(poi_data)\n",
    "            pois.append(poi_data)\n",
    "    return pois\n",
    "\n",
    "\n",
    "def make_pages_and_normalize_input(loop, keys):\n",
    "    if loop == 0:\n",
    "        page = ''\n",
    "    else:\n",
    "        page = 'oa%s-' % (loop * 30)\n",
    "\n",
    "    if 'state' in keys:\n",
    "        state = keys['state']\n",
    "    else:\n",
    "        state = keys['country']\n",
    "\n",
    "    printer('green', 'Country', keys['country'])\n",
    "\n",
    "    if 'name' in keys:\n",
    "        name = keys['name']\n",
    "    else:\n",
    "        name = keys['country']\n",
    "\n",
    "    return page, state, name\n",
    "\n",
    "\n",
    "def crawl_things_to_do_city(keys):\n",
    "    list_dict=[]\n",
    "    for x in range(0, 3):\n",
    "        page, state, name = make_pages_and_normalize_input(x, keys)\n",
    "        url = top_activity_url % (keys['index'], page, name)\n",
    "        dictt=get_poi_list(url, keys, 'things-to-do')\n",
    "        list_dict.extend(dictt)\n",
    "    return list_dict\n",
    "\n",
    "\n",
    "def get_thingsToDo_city(keys):\n",
    "    return crawl_things_to_do_city(keys)\n",
    "\n",
    "\n",
    "def is_exists(url):\n",
    "    return Satl.is_exists(url)\n",
    "\n",
    "\n",
    "def set_data(data):\n",
    "    # if is_exists(data['url']):\n",
    "    #     return False\n",
    "    data['create_date'] = datetime.now()\n",
    "    data['updated'] = False\n",
    "    satl = Satl(data['url'], data=data)\n",
    "    printer('magenta', 'Save', \" %s - %s\" % (satl.pk, satl.get('name')))\n",
    "    satl.save()\n",
    "    # this part writen beacuse of update images\n",
    "    # else:\n",
    "    #     satl = Satl(data['url']).load()\n",
    "    return False\n",
    "\n",
    "def download_uri(cities):\n",
    "    for item in cities:\n",
    "        with open('data/csv/'+item['city']+'.csv', 'r') as csvfile:\n",
    "            dir = os.getcwd() + os.sep+'data/images/'+item['city']  # save directory\n",
    "            if not os.path.exists(dir):\n",
    "                os.mkdir(dir)\n",
    "            csvfile.readline()\n",
    "            lines=csvfile.readlines()\n",
    "            for l in lines:\n",
    "                a=l.split(',')\n",
    "                name=a[0]\n",
    "                uri=a[1]\n",
    "                if os.path.isfile(dir+'/' +name+'_'+ uri.split('/')[-1]):\n",
    "                    return\n",
    "                else:    \n",
    "                    with open(dir+'/' +name+'_'+ uri.split('/')[-1], 'wb') as f:\n",
    "                        f.write(requests.get(uri, stream=True).content)\n",
    "                        print(uri)\n",
    "                        \n",
    "def main():    \n",
    "    for item in cities:\n",
    "        poi_list=get_thingsToDo_city(item)\n",
    "        try:\n",
    "            dir = os.getcwd() + os.sep+'data/csv/'+item['city']+'.csv'  # save directory\n",
    "            with open(dir, 'w',newline='') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=poi_list[0].keys())\n",
    "                writer.writeheader()\n",
    "                for d in poi_list:\n",
    "                    writer.writerow(d)\n",
    "        except IOError:\n",
    "            print(\"I/O error\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    download_uri(cities)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
